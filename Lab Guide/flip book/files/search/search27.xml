Lab 1: Credit Card Fraud - AutoML :@0.120984:0.057725:0.380207:0.057725:0.380207:0.044612:0.120984:0.044612:0.007844:0.008883:0.009662:0.004191:0.009309:0.005026:0.004191:0.009884:0.006472:0.009235:0.009736:0.004024:0.006212:0.004247:0.009884:0.008883:0.006472:0.009625:0.004191:0.008512:0.006472:0.008883:0.009643:0.009680:0.004327:0.005644:0.004233:0.010478:0.009680:0.006212:0.009903:0.015726:0.007666:0.004191
 :@0.120984:0.075969:0.125175:0.075969:0.125175:0.062856:0.120984:0.062856:0.004191
13 :@0.490744:0.939424:0.513682:0.939424:0.513682:0.926311:0.490744:0.926311:0.009474:0.009272:0.004191
 :@0.120984:0.966790:0.125175:0.966790:0.125175:0.953677:0.120984:0.953677:0.004191
 :@0.759894:0.966790:0.764085:0.966790:0.764085:0.953677:0.759894:0.953677:0.004191
What does this mean..? :@0.120984:0.101221:0.300385:0.101221:0.300385:0.088108:0.120984:0.088108:0.016338:0.009643:0.008734:0.006009:0.004191:0.009699:0.009476:0.009254:0.007047:0.004191:0.006101:0.009643:0.004098:0.007010:0.004191:0.014558:0.009254:0.008660:0.009476:0.004618:0.004432:0.008586:0.004191
For the First Generation, the best algorithm pipeline run is: DecisionTree with a scoring of 0.844 and :@0.120984:0.120320:0.867690:0.120320:0.867690:0.107207:0.120984:0.107207:0.008475:0.009662:0.006454:0.004191:0.006101:0.009458:0.009254:0.004191:0.008531:0.004006:0.006398:0.007288:0.005842:0.004191:0.011535:0.009254:0.009476:0.009254:0.006398:0.008734:0.006027:0.004024:0.009662:0.009495:0.004618:0.004191:0.006101:0.009458:0.009254:0.004191:0.009513:0.009068:0.007232:0.006101:0.004191:0.008660:0.004024:0.008698:0.009662:0.006398:0.004098:0.006009:0.009643:0.014539:0.004487:0.009643:0.004098:0.009439:0.009254:0.004024:0.004024:0.009476:0.009254:0.004191:0.006287:0.009476:0.009643:0.004247:0.004024:0.007232:0.004877:0.003987:0.011257:0.009291:0.007881:0.003802:0.007232:0.004024:0.009662:0.009495:0.009050:0.006250:0.009254:0.009311:0.004233:0.012907:0.004024:0.006101:0.009643:0.004191:0.008734:0.003987:0.007232:0.007659:0.009662:0.006454:0.004024:0.009643:0.008698:0.004191:0.009513:0.005619:0.004024:0.009458:0.004529:0.009458:0.009272:0.009524:0.004233:0.008660:0.009476:0.009643:0.004191
accuracy of 0.84746 (figure used to judge the quality of the pipeline) :@0.120984:0.139562:0.633214:0.139562:0.633214:0.126449:0.120984:0.126449:0.008660:0.007881:0.007881:0.009643:0.006398:0.008734:0.007881:0.008178:0.004080:0.009662:0.005638:0.004024:0.009272:0.004659:0.009272:0.009458:0.009272:0.009458:0.009557:0.004031:0.005434:0.005619:0.004024:0.008698:0.009643:0.006250:0.009068:0.004191:0.009699:0.007047:0.009254:0.009476:0.004191:0.006101:0.009662:0.004191:0.004265:0.009476:0.009643:0.008698:0.009068:0.004191:0.006101:0.009643:0.009087:0.004191:0.009513:0.009643:0.008734:0.004006:0.004024:0.006101:0.008178:0.004247:0.009476:0.005619:0.004191:0.006101:0.009458:0.009254:0.004191:0.009699:0.003839:0.009643:0.009291:0.004024:0.004024:0.009476:0.009068:0.005684:0.004191
 :@0.120984:0.158661:0.125175:0.158661:0.125175:0.145548:0.120984:0.145548:0.004191
1. Open the Excel file::@0.151220:0.177760:0.325000:0.177760:0.325000:0.164647:0.151220:0.164647:0.009474:0.004543:0.016212:0.012128:0.009643:0.009254:0.009476:0.004191:0.006101:0.009458:0.009254:0.004270:0.008864:0.007844:0.007659:0.009254:0.004123:0.004233:0.005619:0.004024:0.004024:0.009254:0.004877
 :@0.165128:0.177236:0.170283:0.177236:0.170283:0.162615:0.165128:0.162615:0.005155
 :@0.324817:0.176947:0.335944:0.176947:0.335944:0.163559:0.324817:0.163559:0.011127
C:\Machine--:@0.151220:0.199325:0.284291:0.199325:0.284291:0.185937:0.151220:0.185937:0.011086:0.011080:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011127
Learning\01_Credit_Card\Lab_01_AutoML\output\model_catalogue.xlsx :@0.151220:0.216999:0.883128:0.216999:0.883128:0.203610:0.151220:0.203610:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011080:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011137:0.011086:0.011086:0.011086:0.011120:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011137:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011086:0.011120:0.011086:0.011086:0.011086:0.011086:0.011127
 :@0.879460:0.402425:0.883651:0.402425:0.883651:0.389313:0.879460:0.389313:0.004191
 :@0.120984:0.417961:0.125175:0.417961:0.125175:0.404848:0.120984:0.404848:0.004191
Conclusion:  :@0.120984:0.441907:0.215271:0.441907:0.215271:0.428794:0.120984:0.428794:0.009921:0.009662:0.009643:0.007881:0.004006:0.009643:0.007270:0.004024:0.009476:0.009643:0.004877:0.004049:0.004191
The best pipeline to use (with a 85% accuracy) for this dataset is: based on Decision Trees with a min :@0.120984:0.465709:0.871223:0.465709:0.871223:0.452596:0.120984:0.452596:0.009050:0.009476:0.009254:0.004191:0.009513:0.009068:0.007232:0.006101:0.004191:0.009643:0.004098:0.009421:0.009254:0.004024:0.004024:0.009476:0.009254:0.004191:0.006101:0.009662:0.004024:0.009643:0.007084:0.009450:0.004031:0.005619:0.012907:0.004024:0.006101:0.009643:0.004191:0.008734:0.004191:0.009272:0.009458:0.013111:0.004191:0.008734:0.007881:0.007807:0.009476:0.006398:0.008734:0.007881:0.008030:0.005711:0.004031:0.005619:0.009662:0.006287:0.004233:0.006101:0.009643:0.004006:0.007232:0.004024:0.009643:0.008734:0.006009:0.008660:0.007232:0.009254:0.006101:0.004191:0.003820:0.007344:0.004877:0.004191:0.009643:0.008493:0.007314:0.009068:0.009643:0.004247:0.009476:0.009643:0.004247:0.011090:0.009306:0.007881:0.004006:0.007232:0.004024:0.009476:0.009643:0.004247:0.008864:0.006398:0.009161:0.009031:0.007232:0.004191:0.012963:0.004006:0.006101:0.009643:0.004191:0.008734:0.003987:0.014669:0.004098:0.009643:0.004191
of 8 trees.   :@0.120984:0.484975:0.208820:0.484975:0.208820:0.471862:0.120984:0.471862:0.009662:0.005638:0.004052:0.009474:0.004233:0.006101:0.006194:0.009068:0.009254:0.007047:0.004659:0.004233:0.004031:0.004191
May also be worth looking at KNeighbors Classifier :@0.120984:0.508778:0.499975:0.508778:0.499975:0.495665:0.120984:0.495665:0.015670:0.008734:0.008234:0.004191:0.008734:0.004006:0.007047:0.009662:0.004191:0.009532:0.009254:0.004191:0.012963:0.009458:0.006398:0.006101:0.009643:0.004247:0.004024:0.009662:0.009495:0.008252:0.004024:0.009643:0.008698:0.004043:0.008660:0.006101:0.004191:0.009254:0.011869:0.009254:0.004024:0.008698:0.009643:0.009476:0.009662:0.006268:0.007232:0.004191:0.009921:0.004024:0.008660:0.007047:0.007232:0.004024:0.005619:0.004024:0.009068:0.006559:0.004191
The object of using TPOT is to point you in the right direction for selecting the appropriate algorithm. :@0.120984:0.532723:0.873169:0.532723:0.873169:0.519610:0.120984:0.519610:0.009050:0.009476:0.009254:0.004270:0.009476:0.009643:0.004265:0.009254:0.007881:0.006027:0.004024:0.009662:0.005638:0.004024:0.009643:0.007270:0.003839:0.009643:0.008698:0.004191:0.008957:0.009421:0.011906:0.009050:0.004191:0.004098:0.007028:0.004024:0.006101:0.009662:0.004191:0.009643:0.009662:0.004098:0.009643:0.006101:0.003987:0.008252:0.009662:0.009495:0.004191:0.004098:0.009643:0.004191:0.006101:0.009476:0.009254:0.004024:0.006398:0.004098:0.008698:0.009643:0.006027:0.004191:0.009699:0.003839:0.006250:0.009254:0.007881:0.006027:0.004024:0.009662:0.009643:0.004080:0.005619:0.009662:0.006268:0.004191:0.007103:0.009254:0.004024:0.009254:0.007881:0.006027:0.004024:0.009643:0.008493:0.004191:0.006101:0.009458:0.009254:0.004191:0.008734:0.009458:0.009476:0.006398:0.009717:0.009643:0.006472:0.004024:0.008660:0.005842:0.009254:0.004191:0.008734:0.004006:0.008698:0.009662:0.006398:0.004098:0.006009:0.009476:0.014669:0.004943:0.004191
 :@0.194148:0.556526:0.198339:0.556526:0.198339:0.543413:0.194148:0.543413:0.004191
The results will be different each time you run the :@0.194148:0.580471:0.562973:0.580471:0.562973:0.567358:0.194148:0.567358:0.009050:0.009458:0.008716:0.004191:0.006268:0.008438:0.007232:0.009458:0.004024:0.005842:0.007232:0.004191:0.012963:0.004006:0.004024:0.004024:0.004191:0.009495:0.008716:0.003987:0.009458:0.004024:0.005434:0.005619:0.008716:0.006194:0.008716:0.009402:0.006101:0.004006:0.008929:0.009458:0.007659:0.009458:0.004191:0.006101:0.004024:0.014539:0.008642:0.004191:0.007900:0.009458:0.009458:0.004024:0.006268:0.009254:0.009458:0.004191:0.006101:0.009439:0.008716:0.004191
TPOTClassifier:@0.562907:0.580471:0.668371:0.580471:0.668371:0.567358:0.562907:0.567358:0.008864:0.009421:0.012128:0.008883:0.009921:0.003987:0.008660:0.007232:0.007232:0.004024:0.005619:0.004024:0.009068:0.006398
. :@0.668582:0.580471:0.677207:0.580471:0.677207:0.567358:0.668582:0.567358:0.004435:0.004191
 :@0.194148:0.604416:0.198339:0.604416:0.198339:0.591303:0.194148:0.591303:0.004191